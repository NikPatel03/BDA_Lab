// Databricks notebook source
// DBTITLE 1,Project Introduction 
// MAGIC %md
// MAGIC 
// MAGIC * This project is about **eCommerce** company that sells clothes online. 
// MAGIC * This project is about customers who buys clothes online. 
// MAGIC * The store offers in-store style and clothing advice sessions. 
// MAGIC * Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.
// MAGIC 
// MAGIC We need to predict the future **spending of Customer(ie Revenue for Company )** so business strategies can be made to convert **"Customer" to "Loyalty Customer" **

// COMMAND ----------

// MAGIC %md ### Load Source Data
// MAGIC The data for this Project is provided as a CSV file containing details of **eCommerce customer details** we need to Predict the Yearly Amount Spent . 
// MAGIC 
// MAGIC You will load this data into a DataFrame and display it.

// COMMAND ----------

// MAGIC %scala 
// MAGIC 
// MAGIC import org.apache.spark.sql.Encoders
// MAGIC 
// MAGIC case class eCommerce(Email: String,
// MAGIC                      Avatar: String,
// MAGIC                      Avg_Session_Length: Double,
// MAGIC                      Time_on_App: Double,
// MAGIC                      Time_on_Website: Double,
// MAGIC                      Length_of_Membership: Double,
// MAGIC                      Yearly_Amount_Spent: Double)
// MAGIC 
// MAGIC val eCommerceSchema = Encoders.product[eCommerce].schema
// MAGIC 
// MAGIC val eCommerceDF = spark.read.schema(eCommerceSchema).option("header", "true").csv("/FileStore/tables/ecommerce.csv")
// MAGIC 
// MAGIC display(eCommerceDF)

// COMMAND ----------

// DBTITLE 1,Print Schema
// MAGIC %scala
// MAGIC 
// MAGIC eCommerceDF.printSchema()

// COMMAND ----------

// DBTITLE 1,Display Data
// MAGIC %scala
// MAGIC 
// MAGIC eCommerceDF.show()

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC ###Summary of our Data

// COMMAND ----------

// DBTITLE 1,Finding count, mean, maximum, standard deviation and minimum
// MAGIC %scala
// MAGIC 
// MAGIC eCommerceDF.select("Avg_Session_Length","Time_on_App", "Time_on_Website", "Length_of_Membership", "Yearly_Amount_Spent").describe().show()

// COMMAND ----------

// DBTITLE 1,Creating Temp View from Dataframe 
// MAGIC %scala
// MAGIC 
// MAGIC eCommerceDF.createOrReplaceTempView("EcommerceData")

// COMMAND ----------

// DBTITLE 1,Querying the Temporary View
// MAGIC %sql
// MAGIC 
// MAGIC select * from EcommerceData

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC #Exploratory Data Analysis

// COMMAND ----------

// DBTITLE 1,Histogram of Yearly_Amount_Spent
// MAGIC %sql
// MAGIC 
// MAGIC select Yearly_Amount_Spent from EcommerceData

// COMMAND ----------

// DBTITLE 1,Types of Fashion
// MAGIC %sql
// MAGIC 
// MAGIC select Avatar as Fashion, count(Avatar) from EcommerceData group by Avatar

// COMMAND ----------

// DBTITLE 1,One Visualization to Rule Them All
// MAGIC %sql
// MAGIC 
// MAGIC select Email, Avatar, Avg_Session_Length, Time_on_App, Time_on_Website, Length_of_Membership, Yearly_Amount_Spent from EcommerceData

// COMMAND ----------

// DBTITLE 1,Yearly_Amount_Spent VS Time_on_App
// MAGIC %sql
// MAGIC 
// MAGIC select Yearly_Amount_Spent, Time_on_App from EcommerceData

// COMMAND ----------

// DBTITLE 1,Yearly_Amount_Spent VS Avg_Session_Length
// MAGIC %sql
// MAGIC 
// MAGIC select Yearly_Amount_Spent, Avg_Session_Length from EcommerceData

// COMMAND ----------

// DBTITLE 1,Yearly_Amount_Spent VS Time_on_Website
// MAGIC %sql
// MAGIC 
// MAGIC select Yearly_Amount_Spent, Time_on_Website from EcommerceData

// COMMAND ----------

// MAGIC %md ## Creating a LinearRegression Model
// MAGIC 
// MAGIC In this Project, you will implement a **LinearRegression** that uses features of eCommerce Data details and we will predict **spending of Customer(ie Revenue for Company )**
// MAGIC 
// MAGIC ### Import Spark SQL and Spark ML Libraries
// MAGIC 
// MAGIC First, import the libraries you will need:

// COMMAND ----------

// DBTITLE 1,Importing Library
// MAGIC %scala
// MAGIC 
// MAGIC import org.apache.spark.sql.functions._
// MAGIC import org.apache.spark.sql.Row
// MAGIC import org.apache.spark.sql.types._
// MAGIC 
// MAGIC import org.apache.spark.ml.regression.LinearRegression
// MAGIC import org.apache.spark.ml.feature.VectorAssembler

// COMMAND ----------

// MAGIC %md ###VectorAssembler()
// MAGIC 
// MAGIC VectorAssembler():  is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. 
// MAGIC 
// MAGIC **VectorAssembler** accepts the following input column types: **all numeric types, boolean type, and vector type.** 
// MAGIC 
// MAGIC In each row, the **values of the input columns will be concatenated into a vector** in the specified order.

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC var StringfeatureCol = Array("Email", "Avatar")

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC ###StringIndexer
// MAGIC 
// MAGIC StringIndexer encodes a string column of labels to a column of label indices.

// COMMAND ----------

// DBTITLE 1,Example of StringIndexer
import org.apache.spark.ml.feature.StringIndexer

val df = spark.createDataFrame(
  Seq((0, "a"), (1, "b"), (2, "c"), (3, "a"), (4, "a"), (5, "c"))
).toDF("id", "category")

val indexer = new StringIndexer()
  .setInputCol("category")
  .setOutputCol("categoryIndex")

val indexed = indexer.fit(df).transform(df)

indexed.show()

// COMMAND ----------

// MAGIC %md ### Define the Pipeline
// MAGIC A predictive model often requires multiple stages of feature preparation. 
// MAGIC 
// MAGIC A pipeline consists of a series of *transformer* and *estimator* stages that typically prepare a DataFrame for modeling and then train a predictive model. 
// MAGIC 
// MAGIC In this case, you will create a pipeline with stages:
// MAGIC 
// MAGIC - A **StringIndexer** estimator that converts string values to indexes for categorical features
// MAGIC - A **VectorAssembler** that combines categorical features into a single vector

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC import org.apache.spark.ml.attribute.Attribute
// MAGIC import org.apache.spark.ml.feature.{IndexToString, StringIndexer}
// MAGIC import org.apache.spark.ml.{Pipeline, PipelineModel}
// MAGIC 
// MAGIC val indexers = StringfeatureCol.map { colName =>
// MAGIC   new StringIndexer().setInputCol(colName).setOutputCol(colName + "_indexed")
// MAGIC }
// MAGIC 
// MAGIC val pipeline = new Pipeline()
// MAGIC                     .setStages(indexers)      
// MAGIC 
// MAGIC val FinalecommerceDF = pipeline.fit(eCommerceDF).transform(eCommerceDF)

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC FinalecommerceDF.printSchema()

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC FinalecommerceDF.show()

// COMMAND ----------

// MAGIC %md ### Split the Data
// MAGIC It is common practice when building machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this project, you will use 70% of the data for training, and reserve 30% for testing. 

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC val splits = FinalecommerceDF.randomSplit(Array(0.7, 0.3))
// MAGIC val train = splits(0)
// MAGIC val test = splits(1)
// MAGIC val train_rows = train.count()
// MAGIC val test_rows = test.count()
// MAGIC println("Training Rows: " + train_rows + " Testing Rows: " + test_rows)

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC import org.apache.spark.ml.feature.VectorAssembler
// MAGIC 
// MAGIC val assembler = new VectorAssembler().setInputCols(Array("Email_indexed", "Avatar_indexed", "Avg_Session_Length", "Time_on_App", "Time_on_Website", "Length_of_Membership")).setOutputCol("features")
// MAGIC 
// MAGIC val training = assembler.transform(train).select($"features", $"Yearly_Amount_Spent".alias("label"))
// MAGIC 
// MAGIC training.show()

// COMMAND ----------

// MAGIC %md ### Train a Linear Regression Model 
// MAGIC Next, you need to train a Linear Regression Model using the training data. To do this, create an instance of the Linear Regression Model algorithm you want to use and use its **fit** method to train a model based on the training DataFrame. In this Project, you will use a Linear Regression Model algorithm 

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC import org.apache.spark.ml.regression.LinearRegression
// MAGIC 
// MAGIC val lr = new LinearRegression().setLabelCol("label").setFeaturesCol("features").setMaxIter(10).setRegParam(0.3)
// MAGIC val model = lr.fit(training)
// MAGIC println("Model Trained!")

// COMMAND ----------

// MAGIC %md ### Prepare the Testing Data
// MAGIC Now that you have a trained model, you can test it using the testing data you reserved previously. First, you need to prepare the testing data in the same way as you did the training data by transforming the feature columns into a vector. This time you'll rename the **Yearly_Amount_Spent** column to **trueLabel**.

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC val testing = assembler.transform(test).select($"features", $"Yearly_Amount_Spent".alias("trueLabel"))
// MAGIC 
// MAGIC testing.show()

// COMMAND ----------

// MAGIC %md ### Test the Model
// MAGIC Now you're ready to use the **transform** method of the model to generate some predictions. You can use this approach to predict  **spending of Customer(ie Yearly_Amount_Spent )**  where the label is unknown; but in this case you are using the test data which includes a known true label value, so you can compare the predicted spending of Customer(ie Yearly_Amount_Spent ) . 

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC val prediction = model.transform(testing)
// MAGIC val predicted = prediction.select("features", "prediction", "trueLabel")
// MAGIC predicted.show()

// COMMAND ----------

// MAGIC %md ### Examine the Predicted and Actual Values
// MAGIC You can plot the predicted values against the actual values to see how accurately the model has predicted. In a perfect model, the resulting scatter plot should form a perfect diagonal line with each predicted value being identical to the actual value - in practice, some variance is to be expected.
// MAGIC Run the cells below to create a temporary table from the **predicted** DataFrame and then retrieve the predicted and actual label values using SQL. You can then display the results as a scatter plot, specifying **-** as the function to show the unaggregated values.

// COMMAND ----------

// MAGIC %scala
// MAGIC 
// MAGIC predicted.createOrReplaceTempView("eCommerceData")

// COMMAND ----------

// MAGIC %sql
// MAGIC 
// MAGIC select prediction, trueLabel from eCommerceData

// COMMAND ----------

// MAGIC %md ### Retrieve the Root Mean Square Error (RMSE)
// MAGIC There are a number of metrics used to measure the variance between predicted and actual values. Of these, the root mean square error (RMSE) is a commonly used value that is measured in the same units as the predicted and actual values - so in this case, the RMSE indicates the average number spending of Customer(ie Yearly_Amount_Spent ) values. You can use the **RegressionEvaluator** class to retrieve the RMSE.

// COMMAND ----------

import org.apache.spark.ml.evaluation.RegressionEvaluator

val evaluator = new RegressionEvaluator().setLabelCol("trueLabel").setPredictionCol("prediction").setMetricName("rmse")
val rmse = evaluator.evaluate(prediction)
println("Root Mean Square Error (RMSE): " + (rmse))

// COMMAND ----------


